# Scalability, Programming and Arrays 
---
## Threads Vs Processes 

**Similarities:** 
- Each has its own logical control flow
- Can run concurrently with others (at the same time)

**Differences:** 
- Threads share code and some data (processes don't)
- Process control (creating and reaping) 2x as expensive as thread control
- linux numbers
	- 20k cycles to create and reap a process
	- 10k cycles to create and reap a thread
	 
Processes create a lot of overhead
If you want to run more instructions on the ALU's but you don't need new memory. Cheap option is thread. Can read and write from same memory which means you can make mistakes ie overwrite things. 

## "Nanny" Process

**Coordination**: 
**Task status**: (idle, in progress, completed)
Idle tasks: Get scheduled as workers become available

**Resiliency:** Pings workers periodically to detect failures
- if connection to a remote worker unexpectedly closes or times out, scheduler reroutes all pending computations to other workers.

**Failure Caveats:**
- If you give it a bad func it tries it 3 times and marks it as bad
- Data sent out directly to workers is client responsibility.

## Dask Arrays
We use arrays because they're memory efficient and allow us to interact with low level components.

In Dask's distributed computing framework, a "nanny process" serves a critical role:

1. The nanny process supervises and manages Dask worker processes
2. It monitors worker health and resource usage
3. If a worker crashes or becomes unresponsive, the nanny automatically restarts it
4. This helps maintain the resilience of your Dask cluster during long-running computations

Dask arrays are essentially chunked NumPy arrays that operate in parallel. Key characteristics include:

1. **Chunked Structure**: Dask arrays split large arrays into smaller chunks that fit in memory.
2. **Lazy Evaluation**: Operations on Dask arrays create a task graph rather than executing immediately.
3. **NumPy API Compatibility**: Dask arrays implement most of the NumPy API, making them familiar to use.
4. **Out-of-Core Processing**: They can process datasets larger than available RAM by working on chunks sequentially.
---
# Lecture 2

create a 2D array that combines a product table of 2 1D arrays ie.
comb_prod([2 4], [3 1]) => ([6, 2], [12, 4])



normally:
```
result = []

for x in a:
	row = []
	for y in b:
		row.append(x * y).compute()
	result.append(row)
```
**Map:** maps a function to each element in the dataset, which we can use to replace the function above:

```

```







